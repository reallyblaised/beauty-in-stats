{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77db5001-a402-4755-bfec-c3e3aa2e1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModel\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from data_processor import DataProcessor\n",
    "import torch.nn.functional as F\n",
    "from typing import Union\n",
    "from transformers import ModernBertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1258883a-c435-4cee-a30b-71d311cbc8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ea41ee-8fc0-49ea-bc4a-ce4fa4a4e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"answerdotai/ModernBERT-base\",\n",
    "        output_dir: str = \"/ceph/submit/data/user/b/blaised/mlm_output\",\n",
    "        cache_dir: str = \"/ceph/submit/data/user/b/blaised/cache\",\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cache_dir = cache_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=self.cache_dir\n",
    "        )\n",
    "        \n",
    "        # Initialize model with Flash Attention 2 disabled\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "                torch_dtype=torch.bfloat16,  # More efficient than float32\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                reference_compile=False,\n",
    "                classifier_pooling=\"mean\",\n",
    "            ).to(f\"cuda:{torch.cuda.current_device()}\")\n",
    "        else:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "                reference_compile=False,\n",
    "            )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_dataset: Dataset,\n",
    "        eval_dataset: Dataset = None,\n",
    "        num_train_epochs: int = 3,\n",
    "        per_device_train_batch_size: int = 8,\n",
    "        gradient_accumulation_steps: int = 4,\n",
    "        learning_rate: float = 3e-4,\n",
    "        weight_decay: float = 1e-5\n",
    "    ):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            logging_steps=1,\n",
    "            save_strategy=\"epoch\",\n",
    "            eval_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "            # Performance optimizations\n",
    "            fp16=True,                    # Enable mixed precision training\n",
    "            dataloader_num_workers=4,     # Parallel data loading\n",
    "            dataloader_pin_memory=True,   # Faster data transfer to GPU\n",
    "            optim=\"adamw_torch_fused\",    # Use fused optimizer\n",
    "            lr_scheduler_type=\"cosine\",   # Cosine decay often works well\n",
    "            warmup_ratio=0.1,             # Gradual warmup for first 10% of steps\n",
    "            # Save best model\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "    \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=True, mlm_probability=0.15\n",
    "        )\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "    \n",
    "        trainer.train()\n",
    "        trainer.save_model(f\"{self.output_dir}/final_model\")\n",
    "        self.tokenizer.save_pretrained(f\"{self.output_dir}/final_model\")\n",
    "    \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d7a49c-41e1-43b9-bd9e-6d4b75de7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataProcessor.load_and_process_data(\n",
    "    \"/ceph/submit/data/user/b/blaised/lhcb_corpus/lhcb_papers.pkl\"\n",
    ")\n",
    "texts = data[\"abstract\"].tolist()\n",
    "\n",
    "# Train/eval split\n",
    "np.random.seed(42)\n",
    "eval_size = int(len(texts) * 0.1)\n",
    "eval_indices = np.random.choice(len(texts), eval_size, replace=False)\n",
    "train_indices = [i for i in range(len(texts)) if i not in eval_indices]\n",
    "\n",
    "train_texts = [texts[i] for i in train_indices]\n",
    "eval_texts = [texts[i] for i in eval_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00171ef4-59a9-4d90-a7c8-97f80ce43609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2) Initialize trainer and get tokenizer\n",
    "# ------------------------------------------------------------\n",
    "mlm_trainer = MLMTrainer()\n",
    "tokenizer = mlm_trainer.tokenizer  # Get reference to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92593f1e-16e9-4518-be22-54b2b3fcfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=True,  # ModernBERT unpadded usage\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f707daba-6022-474d-b75a-34b37e1b5f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164dca395528433ebb4c5a8a4f1a503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08863030bb7c44d6b96305cd997fa21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # ------------------------------------------------------------\n",
    "# 3) Prepare and tokenize datasets\n",
    "# ------------------------------------------------------------\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "eval_dataset = Dataset.from_dict({\"text\": eval_texts})\n",
    "\n",
    "# Tokenize using multiple processes\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=False,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2baf40d2-a489-40ed-9648-85d6e595023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 730\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0297713e-244a-4638-8f51-9c8153450a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 16:25, Epoch 27/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.193000</td>\n",
       "      <td>0.578467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.068500</td>\n",
       "      <td>0.574446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.267700</td>\n",
       "      <td>0.591494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.009600</td>\n",
       "      <td>0.574323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.932100</td>\n",
       "      <td>0.539634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.548926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.711600</td>\n",
       "      <td>0.523832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.680500</td>\n",
       "      <td>0.554088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>0.540707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.742200</td>\n",
       "      <td>0.509413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.735900</td>\n",
       "      <td>0.522840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.570825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.430524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.498127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.542063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.464853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>0.544599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>0.496150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.617900</td>\n",
       "      <td>0.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.488293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.516171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>0.490629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.356800</td>\n",
       "      <td>0.483159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.444772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.488072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.473069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.898200</td>\n",
       "      <td>0.492175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/work/submit/blaised/beauty-in-stats/venv/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['decoder.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f0a2b82c580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4) Train the model\n",
    "# ------------------------------------------------------------\n",
    "mlm_trainer.train(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b004a8a-8b44-4ff4-a9f4-888120eb3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModel:\n",
    "    \"\"\"Model loading, tokenisatuionn, and inference.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        cache_dir: str = \"/ceph/submit/data/user/b/blaised/cache\",\n",
    "        device: Union[str, None] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the model.\"\"\"\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.cache_dir,\n",
    "        )\n",
    "\n",
    "        # book the encoding model\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "                #attn_implementation=\"flash_attention_2\",\n",
    "            ).to(\"cuda\")\n",
    "        else:\n",
    "            self.model = ModernBertModel.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "            )\n",
    "\n",
    "        # sanity device check\n",
    "        assert (\n",
    "            self.model.device.type == self.device.type\n",
    "        ), f\"Model is on {self.model.device.type}, but expected {self.device.type}.\"\n",
    "\n",
    "    def encode(self, texts: list[str], batch_size: int = 1) -> torch.Tensor:\n",
    "        \"\"\"Get embeddings for a list of texts.\"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        # Process texts in full-batch mode\n",
    "        self.model.eval()\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "            batch_text = texts[i : i + min(batch_size, len(texts))]\n",
    "\n",
    "            # Tokenize and encode the batch\n",
    "            inputs = self.tokenizer(batch_text, return_tensors=\"pt\").to(self.device) \n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                # Fetch the [CLS] representation in the last embedding layer - following BERT - see ModernBertConfig() in Transformers\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :]  # checked\n",
    "\n",
    "                embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "        return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "    \n",
    "    def mean_pool_encode(self, texts, batch_size=1, prefix: Union[str, None] = None):\n",
    "        \"\"\"Encoding via mean pooling\"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        # for modernbert-embed, we need to preped prefix\n",
    "        if prefix:\n",
    "            texts = [f\"{prefix}: {t}\" for t in texts]\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batched\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize and encode the batch\n",
    "            inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device) # FIXME: truncation, and/or padding\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_embeddings = self.mean_pooling(outputs, inputs['attention_mask'])\n",
    "                batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "                embeddings.append(batch_embeddings.cpu())\n",
    "                \n",
    "        return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307036e2-5349-4052-92ff-df57c18f2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhcb_abstract_dataset = DataProcessor.load_and_process_data(\n",
    "    \"/ceph/submit/data/user/b/blaised/lhcb_corpus/lhcb_papers.pkl\"\n",
    ")\n",
    "\n",
    "abstract_corpus = lhcb_abstract_dataset[\"abstract\"].tolist()\n",
    "working_groups = lhcb_abstract_dataset[\"working_groups\"].tolist()\n",
    "abstract_labels = lhcb_abstract_dataset[\"encoded_wg\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(abstract_corpus)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195789e-54bb-4148-827f-113486d65887",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = EncoderModel(\n",
    "        #model_name=\"/ceph/submit/data/user/b/blaised/mlm_output/final_model\",\n",
    "        model_name=\"answerdotai/ModernBERT-base\",\n",
    "        #model_name=\"nomic-ai/modernbert-embed-base\",\n",
    "        #model_name=\"lightonai/modernbert-embed-large\",\n",
    "        #model_name=\"thellert/physbert_cased\",\n",
    "        device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3c961-719e-4e62-a452-90d0bee0c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = trained_model.mean_pool_encode(abstract_corpus, prefix=None)\n",
    "embeddings_np = embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e03fbc-5811-41b6-97db-07af0fd73172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "class Visualizer:\n",
    "    \"\"\"Handles visualization of embeddings and metrics\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_embeddings(embeddings, labels, method=\"pca\", save_path=\"test.png\"):\n",
    "        # Convert inputs to numpy arrays if they aren't already\n",
    "        embeddings_np = np.array(embeddings)\n",
    "        labels_np = np.array(labels)\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        reducer = PCA(n_components=2) if method.lower() == \"pca\" else umap.UMAP(random_state=42)\n",
    "        reduced = reducer.fit_transform(embeddings_np)\n",
    "        \n",
    "        # Get unique labels and assign colors\n",
    "        unique_labels = np.unique(labels_np)\n",
    "        colors = sns.color_palette(\"Spectral\", n_colors=len(unique_labels))\n",
    "        \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            # Create boolean mask for this label\n",
    "            mask = (labels_np == label)\n",
    "            if np.sum(mask) > 0:  # Only plot if we have points for this label\n",
    "                plt.scatter(reduced[mask, 0], reduced[mask, 1],\n",
    "                           c=colors[i], label=str(label), alpha=0.75)\n",
    "        \n",
    "        plt.title(f\"LHCb Abstracts Embeddings ({method.upper()})\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 0.85), loc=\"upper left\", bbox_transform=plt.gcf().transFigure)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e032b-67a1-4292-9f9e-38988c03832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizer().plot_embeddings(\n",
    "    embeddings_np, working_groups, method=\"umap\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0816f75-a5f7-4f75-bc1d-6d49bb15653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import MetricsCalculator\n",
    "\n",
    "metrics_calc = MetricsCalculator()\n",
    "\n",
    "print(\"\\nComputing clustering metrics...\")\n",
    "clustering_results = metrics_calc.compute_clustering_metrics(\n",
    "    embeddings=embeddings_np, labels=working_groups\n",
    ")\n",
    "\n",
    "print(f\"NMI score: {clustering_results['nmi_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a73d04-1143-400c-a584-cdc626b0df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute group metrics\n",
    "print(\"\\nComputing group metrics...\")\n",
    "group_metrics = metrics_calc.compute_group_metrics(\n",
    "    embeddings=embeddings_np, groups=working_groups\n",
    ")\n",
    "\n",
    "print(\"\\nGroup statistics:\")\n",
    "for group, metrics in group_metrics.items():\n",
    "    print(f\"\\nGroup: {group}\")\n",
    "    print(f\"Count: {metrics['count']}\")\n",
    "    print(f\"Average similarity: {metrics['avg_similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394d13b-6019-40e2-8977-107155a8af6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
